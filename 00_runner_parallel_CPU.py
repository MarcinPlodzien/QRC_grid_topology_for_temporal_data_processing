"""
Parallel Simulation Runner (CPU)
================================
Author: Marcin Plodzien

This script orchestrates the high-throughput execution of QRC simulations.
It leverages Python's `ProcessPoolExecutor` to distribute simulation batches across CPU cores.

Key Features:
-------------
1.  **CPU Optimization**: Forces JAX to use the CPU backend (`JAX_PLATFORM_NAME='cpu'`) preventing 
    memory competition on GPU during parallel runs.
2.  **Dynamic Configuration**: Loads the experiment configuration module at runtime via string argument.
3.  **Data Persistence**: Collects all results into a single Pandas DataFrame and saves as Pickle.
4.  **Robustness**: Wraps individual worker tasks in try/except blocks to prevent single-failure crashes.

Usage:
------
    python3 00_runner_parallel_CPU.py 01_config_qrc_ladder.py [optional: --serial]

"""

import os
import sys
import time
import pickle
import numpy as np
import pandas as pd
import jax
import jax.numpy as jnp
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import importlib

# Import project modules
import utils.engine as ue
# from sklearn.linear_model import LinearRegression # Removed to avoid conflict with parallel processing

# Force CPU (Safe Mode)
# This prevents JAX from attempting to pre-allocate all GPU memory in each subprocess.
os.environ["JAX_PLATFORM_NAME"] = "cpu"
os.environ["XLA_FLAGS"] = "--xla_force_host_platform_device_count=1"

def load_and_prep_data(filename, n_points, mode='batch_data_input', L=3):
    """
    Loads time-series data and prepares it for injection into the quantum system.
    
    Args:
        filename (str): Path to dataset (e.g., 'santafe.txt').
        n_points (int): Number of time steps to load.
        mode (str): Injection strategy.
            - 'single_data_input': Encodes u(t) identically across all L sites. Shape (T, L).
            - 'batch_data_input': Encodes a sliding window [u(t), u(t-1), ...] across sites.
        L (int): Size of the Input Rail.
        
    Returns:
        jnp.ndarray: Prepared input batch of shape (T, L).
    """
    # Load
    try:
        raw = np.loadtxt(filename)
    except OSError:
        raw = np.loadtxt(f"datasets/{filename}")
        
    # Min-Max Normalization to [0, 1]
    ts = (raw - raw.min()) / (raw.max() - raw.min())
    ts = ts[:n_points]
    
    # Prep
    T = len(ts)
    if mode == 'single_data_input':
        # Broadcast scalar u(t) to vector [u(t), u(t), ..., u(t)]
        # Shape: (T, L)
        inp = np.repeat(ts[:, None], L, axis=1)
        
    elif mode == 'batch_data_input':
        # Sliding Window / Delay Embedding
        # Site 0 gets u(t), Site 1 gets u(t-1), etc.
        padded = np.pad(ts, (L, 0), constant_values=0.0)
        windows = []
        for t in range(T):
            # Window [t+1 : t+L+1] reversed gives [u(t), u(t-1)...]
            win = padded[t+1 : t+L+1][::-1]
            windows.append(win)
        inp = np.array(windows)
        
    return jnp.array(inp)

def worker_fn(config):
    """
    Worker Function for a Single Simulation Batch.
    Executed in a separate process.
    
    Workflow:
    1.  Unpack parameters from `config` dictionary.
    2.  Load Dataset.
    3.  Generate Random Keys (Realizations).
    4.  Call `utils.engine.simulation_kernel` via `jax.vmap` to run N_realizations in parallel (SIMD).
    5.  Collect results and format into a Pandas DataFrame.
    
    Args:
        config (dict): Single configuration dictionary generated by `01_config_*.py`.
        
    Returns:
        dict: {'status': 'success', 'data': DataFrame} or {'status': 'error', 'error': msg}
    """
    try:
        # Unpack Config
        L = config['L']
        t_evol = config['t_evol']
        dt = config['dt']
        h_mag = config['h_mag']
        N_real = config['n_realizations']
        # Extract ham_config
        hc = config['ham_config']
        topology = hc.get('topology', 'ladder')
        
        # Data Loading
        d_mode = config['data_input_type']
        target_dataset = config['target_dataset']
        # T_max is fixed to 4000 for standard validation runs
        inp_jax = load_and_prep_data(target_dataset, 4000, d_mode, L)
        
        # Observables
        observables = tuple(config.get('observables', ('Z',)))
        prepare_meas = config.get('prepare_measurement', True)
        
        # Random Keys
        seed_base = config['seed_base']
        real_start = config['realization_start']
        master_key = jax.random.PRNGKey(seed_base + real_start)
        keys = jax.random.split(master_key, N_real)
        
        # DISPATCH LOGIC
        if topology == 'multirail':
            n_rails = config['n_rails']
            J_rails = tuple(hc['J_rails'])
            J_rungs = tuple(hc['J_rungs'])
            field_rails = tuple(hc['field_rails'])
            
            # Extract measured rails (defaults to ALL rails if None or missing)
            # Default: Rails 0 to n_rails-1
            mr_config = config.get('measured_rails', None)
            if mr_config is None:
                measured_rails = tuple(range(n_rails))
            else:
                measured_rails = tuple(mr_config)
            
            # VMAP: axes match simulation_kernel_multirail (17 args now)
            # key_seed(0), input_batch(None), L(None), t(None), dt(None), h(None), 
            # Jrail(None), Jrung(None), field(None), n_rails(None), 
            # target(None), top(None), meas_rails(None), int(None), obs(None), prep(None), field_dis(None)
            
            sim_multirail_batch = jax.vmap(ue.simulation_kernel_multirail, 
                                           in_axes=(0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None))
            
            res_ens = sim_multirail_batch(
                keys, inp_jax,
                L, t_evol, dt, h_mag,
                J_rails, J_rungs, field_rails, n_rails,
                config['input_state_type'], topology, measured_rails,
                config.get('integration_method', 'trotter'),
                observables, prepare_meas,
                config.get('field_disorder', True)
            )
            
        else:
            # Fallback to Ladder / Legacy
            J_rail_left = tuple(hc.get('J_rail_left', (0.0, 0.0, 0.0)))
            J_rail_right = tuple(hc.get('J_rail_right', (0.0, 0.0, 0.0)))
            J_rung = tuple(hc.get('J_rung', (0.0, 0.0, 0.0)))
            J_all = tuple(hc.get('J_all', (0.0, 0.0, 0.0)))
            
            if 'field' in hc:
                 fL = fR = tuple(hc['field'])
            else:
                 fL = tuple(hc.get('field_L', (0.0, 0.0, 0.0)))
                 fR = tuple(hc.get('field_R', (0.0, 0.0, 0.0)))
                 
            simulation_batch = jax.vmap(ue.simulation_kernel, 
                                        in_axes=(0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None))
                                        
            res_ens = simulation_batch(
                keys, inp_jax, 
                L, t_evol, dt, h_mag, 
                J_rail_left, J_rail_right, J_rung, J_all, 
                fL, fR, 
                config['input_state_type'], topology, 
                config.get('integration_method', 'trotter'), 
                observables, prepare_meas,
                config.get('field_disorder', True)
            )
        
        # Force computation and convert to NumPy
        res_ens.block_until_ready()
        res_np = np.array(res_ens) # Shape: (N_real, T, n_observables)
        
        # Metadata Extraction (for DataFrame)
        u_k = inp_jax[:, 0] # Extract input signal for reference
        
        dfs = []
        for r in range(N_real):
            # Unpack flat vector into Long DataFrame
            # Result shape: (T, Obs_Flat_Dim)
            res_r = res_np[r] 
            
            # We need to construct a dict of lists to create the DF
            data_dict = {
                'k': [], 
                's_k': [],
                't_evol': [],
                'h_mag': [],
                'rail_idx': [],
                'qubit_idx': [],
                # Observables will be added dynamically
            }
            
            # Helper to initialize lists for obs
            obs_storage = {} # { 'Z_exp_val': [], 'XX_exp_val': [], ... }
            
            # Map canonical names requested by user
            # Z_local_mean -> Z_exp_val
            # X_local_mean -> X_exp_val
            # ZZ_local_mean -> ZZ_exp_val
            obs_map = {
                'Z_local_mean': 'Z_exp_val',
                'X_local_mean': 'X_exp_val',
                'Y_local_mean': 'Y_exp_val',
                'Z_local_std': 'Z_std_val',
                'X_local_std': 'X_std_val',
                'ZZ_local_mean': 'ZZ_exp_val',
                'XX_local_mean': 'XX_exp_val'
            }
            
            # Prepare config for unpacking
            # Engine flattens: For each Obs: For each Rail: For each Site
            
            # We iterate time steps to build rows
            # Optimization: Build arrays first then flatten
            
            T_steps = res_r.shape[0]
            
            # 1. Expand Metadata to (T, N_rails, L)
            # Actually, let's process per-observable block
            
            mr_config = config.get('measured_rails', None)
            if mr_config is None:
                measured_rails = tuple(range(n_rails))
            else:
                measured_rails = tuple(mr_config)
            
            n_mr = len(measured_rails)
            
            # Pre-allocate Long DF coordinate columns
            # Total Rows = T * (n_mr * L)
            total_rows = T_steps * n_mr * L
            
            # Coordinates
            # Time: Repeat each t step (n_mr*L) times
            # k_col = np.repeat(np.arange(T_steps), n_mr * L)
            
            # Or easier: Make a Base MultiIndex (k, rail, qubit)
            iterables = [np.arange(T_steps), measured_rails, np.arange(L)]
            idx = pd.MultiIndex.from_product(iterables, names=['k', 'rail_idx', 'qubit_idx'])
            long_df = pd.DataFrame(index=idx).reset_index()
            
            # Current pointer in flat vector
            ptr = 0
            
            for obs_name in observables:
                target_col = obs_map.get(obs_name, obs_name)
                
                if 'local' in obs_name:
                    # Shape: (T, n_mr * L_eff)
                    # L_eff = L for Z,X. L-1 for ZZ,XX.
                    
                    is_corr = any(x in obs_name for x in ['ZZ', 'XX', 'YY'])
                    sites_per_rail = L - 1 if is_corr else L
                    block_size = n_mr * sites_per_rail
                    
                    # Extract block for all time steps
                    # res_r: (T, Flat)
                    # block: (T, block_size)
                    block = res_r[:, ptr : ptr + block_size]
                    ptr += block_size
                    
                    # Reshape to (T, n_mr, sites_per_rail)
                    block_reshaped = block.reshape(T_steps, n_mr, sites_per_rail)
                    
                    # We need to map this to the Long DF (T, n_mr, L)
                    # If correlation, we have missing values for last site L-1
                    
                    flat_vals = []
                    
                    # It's faster to assign via indexing if we sort Long DF
                    # Long DF is sorted by k, then rail, then qubit
                    
                    if is_corr:
                        # We have data for sites 0..L-2. Site L-1 is NaN.
                        # Create full array (T, n_mr, L) with NaNs
                        full_arr = np.full((T_steps, n_mr, L), np.nan)
                        full_arr[:, :, :sites_per_rail] = block_reshaped
                        
                        # Flatten to match DF order (k dominant, then rail, then qubit)
                        # We used pd.MultiIndex.from_product([T, Rails, Sites])
                        # So flatten order should be C-style if arrays match
                        
                        vals_flat = full_arr.flatten() 
                        long_df[target_col] = vals_flat
                        
                    else:
                        # Full data
                        vals_flat = block_reshaped.flatten()
                        long_df[target_col] = vals_flat
                        
                else:
                    # Total / Scalar
                    # Just replicate across all rows for that time step? 
                    # Or keep as separate DF? 
                    # User requested 'rail_idx', 'qubit_idx'. 
                    # Scalars like 'Z_total_mean' (sum over measured rails) 
                    # are technically properties of the 'System' (or subsystem).
                    # We can assign them to all rows (redundant) or just ignore 
                    # if user only wants Local.
                    # Let's assign redundant for now or skip. User emphasized local columns.
                    # We advance pointer!
                    ptr += 1
                    
            # Add Static Metadata
            long_df['s_k'] = long_df['k'].apply(lambda k: u_k[k] if k < len(u_k) else np.nan)
            long_df['realization'] = r
            long_df['t_evol'] = t_evol
            long_df['h_mag'] = h_mag
            long_df['config_name'] = config['name']
            long_df['field_disorder'] = config.get('field_disorder', True)
            
            # Calculate X^2_exp_val if X_exp_val and X_std_val exist
            if 'X_exp_val' in long_df.columns and 'X_std_val' in long_df.columns:
                 # Var = E[X^2] - (E[X])^2  => E[X^2] = Var + E[X]^2
                 # Std = sqrt(Var)
                 long_df['X^2_exp_val'] = long_df['X_std_val']**2 + long_df['X_exp_val']**2
            
            # Add specific config labels
            pnames = config.get('param_names', {})
            long_df['j_rung_name'] = pnames.get('J_rungs', 'Unknown')
            if topology == 'multirail':
                 long_df['j_rails_name'] = pnames.get('J_rails', 'Unknown')
            
            # Missing Metadata from previous refactor
            long_df['input_state_type'] = config.get('input_state_type', 'unknown')
            long_df['data_input_type'] = config.get('data_input_type', 'unknown')
            long_df['topology'] = topology
            long_df['dt'] = dt
            
            dfs.append(long_df)
            
        full_df = pd.concat(dfs, ignore_index=True)
        
        # --- SAVE INDIVIDUAL PICKLE ---
        # User requested separate files per configuration
        # NEW: Save to 'data' subfolder
        batch_dir = f"results/{config.get('config_name_batch', 'UnknownConfig')}/data"
        os.makedirs(batch_dir, exist_ok=True)
        
        # New Standardized Filename
        import utils.helpers as uh
        filename = uh.generate_result_filename(config, iter_idx=real_start)
        pkl_path = f"{batch_dir}/{filename}"
        
        full_df.to_pickle(pkl_path)
        # -----------------------------
        
        return {'status': 'success', 'data': full_df, 'path': pkl_path}
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {'status': 'error', 'error': str(e)}

def main():
    start_time = time.time()
    
    # Argument Parsing
    config_name = sys.argv[1] if len(sys.argv) > 1 else "01_config_qrc_ladder.py"

    if config_name.endswith('.py'):
        config_name = config_name[:-3]
    try:
        config_module = importlib.import_module(config_name)
    except ModuleNotFoundError:
        print(f"Config '{config_name}' not found.")
        return

    print(f"Loading Configuration: {config_name}...")
    tasks = config_module.generate_configs()
    print(f"Generated {len(tasks)} batches.")
    
    # Determine Execution Mode
    max_workers = getattr(config_module, 'MAX_WORKERS', 1)
    
    results_list = []
    
    if max_workers > 1:
        print(f"Running PARALLEL with {max_workers} workers.")
        # Use 'spawn' for JAX compatibility
        import multiprocessing
        ctx = multiprocessing.get_context('spawn')
        with ProcessPoolExecutor(max_workers=max_workers, mp_context=ctx) as executor:
            futures = [executor.submit(worker_fn, t) for t in tasks]
            for fut in tqdm(as_completed(futures), total=len(futures)):
                res = fut.result()
                if res['status'] == 'success':
                    results_list.append(res['data'])
                else:
                    print(f"Error: {res['error']}")
    else:
        print("Running SERIAL (Debugging Mode).")
        for t in tqdm(tasks):
             res = worker_fn(t)
             if res['status'] == 'success':
                 results_list.append(res['data'])
             else:
                 print(f"Error: {res['error']}")
                 
    # Merge & Save
    if results_list:
        print("Merging DataFrames...")
        final_df = pd.concat(results_list, ignore_index=True)
        
        # NEW: Consolidated file also goes to data subfolder
        out_dir = f"results/{config_module.CONFIG_NAME}/data"
        os.makedirs(out_dir, exist_ok=True)
        out_path = f"{out_dir}/collected_simulation_results.pkl"
        
        print(f"Saving to {out_path}...")
        final_df.to_pickle(out_path)
        print("Done.")
    else:
        print("No results collected (Sim Failed?).")

    total_time = time.time() - start_time
    hours = total_time // 3600
    minutes = (total_time % 3600) // 60
    seconds = total_time % 60
    print(f"\n--- Execution Completed in {total_time:.2f} seconds ({int(hours)}h {int(minutes)}m {seconds:.2f}s) ---")

if __name__ == "__main__":
    main()
